{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Thesis.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "metadata": {
    "id": "2bL8DLlSw0Ma",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "# memory footprint support libraries/code\n",
    "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "!pip install gputil\n",
    "!pip install psutil\n",
    "!pip install humanize\n",
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
    "gpu = GPUs[0]\n",
    "def printm():\n",
    " process = psutil.Process(os.getpid())\n",
    " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "printm()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "u6YtuSv2Szbm",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "ePGq59kkSlu9",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "**Make Dirs**"
   ]
  },
  {
   "metadata": {
    "id": "th_Xs62dS3T8",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "  import os\n",
    "  os.mkdir('training')\n",
    "  os.mkdir('testing')\n",
    "except:\n",
    "  pass"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "TWNVjSsuSpAE",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": []
  },
  {
   "metadata": {
    "id": "cGd3q-onSpwD",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "outputId": "8de9ed00-7265-4760-efaf-39b077b737aa",
    "executionInfo": {
     "status": "error",
     "timestamp": 1556637786601,
     "user_tz": 360,
     "elapsed": 4251,
     "user": {
      "displayName": "Christian L",
      "photoUrl": "",
      "userId": "10984135569297592018"
     }
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.set_random_seed(1)\n",
    "np.random.seed(3)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import class_weight\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers\n",
    "from keras import Sequential\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier as XGB\n",
    "\n",
    "# AI LIBRARY - sklearn or keras\n",
    "aiLib = 'xgboost'\n",
    "\n",
    "# DIRECTORY SETTINGS\n",
    "testingDir = 'testing'\n",
    "trainingDir = 'training'\n",
    "modelsDir = {'keras': 'keras_models', 'sklearn': 'sklearn_models', }\n",
    "modelExt = {'keras': 'h5', 'sklearn': 'joblib', }\n",
    "\n",
    "lat = 'LAT_016'\n",
    "long = 'LONG_017'\n",
    "deck = 'DECK_COND_058'\n",
    "superstructure = 'SUPERSTRUCTURE_COND_059'\n",
    "substructure = 'SUBSTRUCTURE_COND_060'\n",
    "channel = 'CHANNEL_COND_061'\n",
    "culvert = 'CULVERT_COND_062'\n",
    "structNum = 'STRUCTURE_NUMBER_008'\n",
    "\n",
    "# AI SETTINGS\n",
    "epochs = 100\n",
    "batchSize = 32  # * 21  # 672\n",
    "encodeOutput = True if aiLib == 'keras' else True\n",
    "print('== GLOBAL SETTINGS ==\\n'\n",
    "      ' o aiLib: {}\\n'\n",
    "      ' o epochs: {}\\n'\n",
    "      ' o batchSize: {}\\n'\n",
    "      '====================='.format(aiLib, epochs, batchSize, ))\n",
    "\n",
    "# PLOT CONFIG\n",
    "el = {\n",
    "    'STRUCTURE_KIND_043A': 10,\n",
    "    'STRUCTURE_TYPE_043B': 23,\n",
    "    'DECK_COND_058': 10,\n",
    "    'DESIGN_LOAD_031': 10,\n",
    "    'SERVICE_LEVEL_005C': 9,\n",
    "    'SURFACE_TYPE_108A': 10,\n",
    "    'DECK_STRUCTURE_TYPE_107': 9,\n",
    "    'MEMBRANE_TYPE_108B': 10,\n",
    "    'DECK_PROTECTION_108C': 10\n",
    "}\n",
    "\n",
    "categoricalCols = [\n",
    "    'STRUCTURE_KIND_043A',  # 10 element vector\n",
    "    'STRUCTURE_TYPE_043B',  # 23 element vector\n",
    "    # 'STRUCTURE_FLARED_035',\n",
    "    # 'DECK_STRUCTURE_TYPE_107',\n",
    "    # 'SURFACE_TYPE_108A',\n",
    "    #'MEMBRANE_TYPE_108B',  # Stupid NBI\n",
    "    #'SERVICE_LEVEL_005C',\n",
    "    #'DECK_PROTECTION_108C',  # Stupid NBI\n",
    "    # 'PIER_PROTECTION_111',\n",
    "    #'DESIGN_LOAD_031',\n",
    "]\n",
    "numericalCols = [\n",
    "    #'ADT_029',\n",
    "    # 'YEAR_ADT_030',\n",
    "    # 'DECK_WIDTH_MT_052',\n",
    "    # 'MAX_SPAN_LEN_MT_048',\n",
    "    # 'PERCENT_ADT_TRUCK_109',\n",
    "    'YEAR_BUILT_027',\n",
    "    #'YEAR_RECONSTRUCTED_106',\n",
    "    #'LAT_016',\n",
    "    #'LONG_017',\n",
    "    # 'DEGREES_SKEW_034',\n",
    "    # 'MIN_VERT_CLR_010',\n",
    "]\n",
    "con = {\n",
    "    'DECK_COND_058': 'Deck',\n",
    "    'SUPERSTRUCTURE_COND_059': 'Superstructure',\n",
    "    'SUBSTRUCTURE_COND_060': 'Substructure',\n",
    "    'STRUCTURE_KIND_043A': 'Kind',\n",
    "    'STRUCTURE_TYPE_043B': 'Type',\n",
    "    'CHANNEL_COND_061': 'Channel',\n",
    "    'CULVERT_COND_062': 'Culvert',\n",
    "    'ADT_029': 'ADT',\n",
    "    # 'YEAR_ADT_030': '',\n",
    "    'PERCENT_ADT_TRUCK_109': '% ADT Trucks',\n",
    "    'YEAR_BUILT_027': 'Year Built',\n",
    "    'YEAR_RECONSTRUCTED_106': 'Year Reconstructed',\n",
    "    'LAT_016': 'Latitude',\n",
    "    'LONG_017': 'Longitude',\n",
    "    'DECK_WIDTH_MT_052': 'Deck Width (m)',\n",
    "    'MAX_SPAN_LEN_MT_048': 'Max Span Length (m)',\n",
    "    'DEGREES_SKEW_034': 'Skew (degrees)',\n",
    "    'MIN_VERT_CLR_010': 'Min Vertical Clearance (m)',\n",
    "    'DESIGN_LOAD_031': 'Design Load',\n",
    "    'SERVICE_LEVEL_005C': 'Service Level',\n",
    "    'SURFACE_TYPE_108A': 'Surface Type',\n",
    "    'DECK_STRUCTURE_TYPE_107': 'Deck Type',\n",
    "    'MEMBRANE_TYPE_108B': 'Membrane Type',\n",
    "    'DECK_PROTECTION_108C': 'Deck Protection',\n",
    "}\n",
    "invcon = {}\n",
    "for key in con:\n",
    "    invcon[con[key]] = key\n",
    "conditionCols = ['DECK_COND_058', 'SUPERSTRUCTURE_COND_059', 'SUBSTRUCTURE_COND_060']\n",
    "\n",
    "pCols = [structNum, deck] + numericalCols + categoricalCols\n",
    "cCols = [structNum, deck]\n",
    "\n",
    "\n",
    "def get_files(path: str, fullPath=True):\n",
    "    return [os.path.join(path, f) if fullPath else f for f in next(os.walk(path))[2]]\n",
    "\n",
    "\n",
    "def path_end(path):\n",
    "    return os.path.basename(os.path.normpath(path))\n",
    "\n",
    "\n",
    "def strip_ext(filename):\n",
    "    return filename.split('.')[0]\n",
    "\n",
    "\n",
    "def encode(item, value):\n",
    "    s = np.zeros(el[item])\n",
    "    s[int(value)] = 1\n",
    "    return s.reshape(1, -1)\n",
    "\n",
    "\n",
    "def munge(df, year=False):\n",
    "    encoded = df[~df.index.duplicated(keep='first')]\n",
    "    for col in encoded.columns:\n",
    "        if 'Deck' not in col:\n",
    "          invcol = invcon[col]\n",
    "          if invcol in el.keys():\n",
    "              try:\n",
    "                  encoded[col] = encoded[col].astype(np.int8)\n",
    "              except:\n",
    "                  pass\n",
    "              encoded[col] = pd.Categorical(encoded[col])\n",
    "              dummies = pd.get_dummies(encoded[col], prefix=col)\n",
    "              encoded = pd.concat([encoded, dummies], axis=1).drop([col], axis=1)\n",
    "    if year:\n",
    "        ecol = encoded.columns\n",
    "        if 'Year Built' in ecol:\n",
    "            encoded['Year Built'] = year - encoded['Year Built']\n",
    "            encoded = encoded.rename(columns={'Year Built': 'Age'})\n",
    "        if 'Year Reconstructed' in ecol:\n",
    "            encoded['Year Reconstructed'] = year - encoded['Year Reconstructed']\n",
    "            encoded = encoded.rename(columns={'Year Reconstructed': 'Last Repair'})\n",
    "    # result['ADT'] = result['ADT'] / (result['Deck Width (m)'] * result['Max Span Length (m)'])\n",
    "    # result = result.drop(['Deck Width (m)', 'Max Span Length (m)'], axis=1)\n",
    "    # 'ADT': 'Capacity',\n",
    "    # })\n",
    "    return encoded\n",
    "\n",
    "\n",
    "def get_model(inputSize, classWeight):\n",
    "    if aiLib == 'keras':\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=1000,\n",
    "                        activation='tanh',\n",
    "                        input_shape=(inputSize,),\n",
    "                        kernel_initializer='lecun_normal',\n",
    "                        kernel_regularizer=regularizers.l2(0.01),\n",
    "                        ))\n",
    "        model.add(Dropout(rate=0.5))\n",
    "        model.add(Dense(units=1000,\n",
    "                        activation='tanh',\n",
    "                        bias_initializer='lecun_normal',\n",
    "                        bias_regularizer=regularizers.l2(0.01)\n",
    "                        ))\n",
    "        model.add(Dropout(rate=0.5))\n",
    "        model.add(Dense(units=10, activation='softmax'))\n",
    "        sgd = SGD(lr=1, clipvalue=0.5, decay=1, momentum=0.5, nesterov=True)\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='categorical_crossentropy')\n",
    "    elif aiLib == 'sklearn':\n",
    "        model = SVC(probability=True, class_weight=classWeight)\n",
    "    elif aiLib == 'xgboost':\n",
    "        model = XGB(probability=True, class_weight=classWeight,\n",
    "                   eta=1e-3, objective='multi:softmax', num_class=10,\n",
    "                   max_depth=20)\n",
    "    return model\n",
    "\n",
    "\n",
    "n = []\n",
    "\n",
    "\n",
    "def read(i, xL, yL, files):\n",
    "    f = files[i - 1]\n",
    "    year = int(strip_ext(path_end(f))[-4:])\n",
    "    _x = pd.read_csv(files[i - 1], usecols=pCols, na_values=['N'] + n).set_index('STRUCTURE_NUMBER_008')\n",
    "    _x = _x.dropna(axis=0).rename(columns=con).add_suffix('_{}_pre'.format(year))\n",
    "    _x = _x[~_x.index.duplicated(keep='first')]\n",
    "    _x.index += '_{}'.format(year)\n",
    "\n",
    "    _y = pd.read_csv(files[i], usecols=cCols, na_values=['N'] + n).set_index('STRUCTURE_NUMBER_008')\n",
    "    _y = _y.dropna(axis=0).rename(columns=con).add_suffix('_{}_cur'.format(year))\n",
    "    _y = _y[~_y.index.duplicated(keep='first')]\n",
    "    _y.index += '_{}'.format(year)\n",
    "    master = pd.concat([_y, _x], axis=1, join='inner')\n",
    "    previous = master[_x.columns].rename(columns=lambda z: str(z)[:-9])\n",
    "    del _x\n",
    "    current = master[_y.columns].rename(columns=lambda z: str(z)[:-9])\n",
    "    del _y\n",
    "    xL.append(previous)\n",
    "    yL.append(current)\n",
    "    return xL, yL\n",
    "\n",
    "\n",
    "def enter_the_matrix():\n",
    "    for col in pCols:\n",
    "        print(col)\n",
    "    trainFiles = get_files(trainingDir)\n",
    "    testFiles = get_files(testingDir)\n",
    "    fullSet = trainFiles + testFiles\n",
    "    xL, yL, xL2, yL2 = [], [], [], []\n",
    "    for i, _ in enumerate(fullSet):\n",
    "        if i > 0:\n",
    "            xL, yL = read(i, xL, yL, fullSet)\n",
    "    P = munge(pd.concat(xL))\n",
    "    print(list(P.columns))\n",
    "    P.drop(['Kind_0', 'Type_0'], axis=1)\n",
    "    print(list(P.columns))\n",
    "    C = pd.concat(yL).astype('int8')\n",
    "    classWeigt = class_weight.compute_sample_weight('balanced', C)\n",
    "    '''classWeigt = class_weight.compute_class_weight('balanced', \n",
    "                                                   np.unique(C[C.columns[0]]),\n",
    "                                                   C)'''\n",
    "    C = munge(C) if encodeOutput else C\n",
    "\n",
    "    trainP = P[~P.index.str.contains(\"_201\")]\n",
    "    trainC = C[~C.index.str.contains(\"_201\")]\n",
    "\n",
    "    testP = P[~P.index.str.contains(\"_19\")]\n",
    "    testC = C[~C.index.str.contains(\"_19\")]\n",
    "    testP = testP[~testP.index.str.contains(\"_200\")]\n",
    "    testC = testC[~testC.index.str.contains(\"_200\")]\n",
    "    inputSize = len(trainP.columns)\n",
    "    trainP, trainC = trainP.to_numpy(), trainC.to_numpy()\n",
    "    testP, testC = testP.to_numpy(), testC.to_numpy()\n",
    "    print(inputSize)\n",
    "    print(len(trainC))\n",
    "    print(len(testC))\n",
    "    model = get_model(inputSize=inputSize, classWeight=classWeigt)\n",
    "    model.fit(trainP, trainC)  # , batch_size=32, class_weight=classWeigt, epochs=10)\n",
    "    print(validate(trainP, trainC, model))\n",
    "    print(validate(testP, testC, model))\n",
    "\n",
    "\n",
    "def validate(P, C, model):\n",
    "    gucci, bacci = [], []\n",
    "    _p = model.predict_proba(P)\n",
    "    p = np.vstack([encode(deck, i) for i in np.argmax(_p, axis=1)])\n",
    "    print(np.unique(p, axis=0))\n",
    "    c = np.vstack([encode(deck, i[0]) for i in C])\n",
    "    for a, b in zip(c, p):\n",
    "        gucci.append(0) if np.array_equal(a, b) else bacci.append(0)\n",
    "    acc = len(gucci) / (len(gucci) + len(bacci))\n",
    "    print(len(gucci), len(bacci))\n",
    "    return acc\n",
    "\n",
    "\n",
    "enter_the_matrix()\n"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "== GLOBAL SETTINGS ==\n",
      " o aiLib: xgboost\n",
      " o epochs: 100\n",
      " o batchSize: 32\n",
      "=====================\n",
      "STRUCTURE_NUMBER_008\n",
      "DECK_COND_058\n",
      "YEAR_BUILT_027\n",
      "STRUCTURE_KIND_043A\n",
      "STRUCTURE_TYPE_043B\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "error",
     "ename": "StopIteration",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mStopIteration\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-e4acd8574dac>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m    270\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    271\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 272\u001B[0;31m \u001B[0menter_the_matrix\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-1-e4acd8574dac>\u001B[0m in \u001B[0;36menter_the_matrix\u001B[0;34m()\u001B[0m\n\u001B[1;32m    220\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mcol\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpCols\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    221\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 222\u001B[0;31m     \u001B[0mtrainFiles\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_files\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrainingDir\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    223\u001B[0m     \u001B[0mtestFiles\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_files\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtestingDir\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    224\u001B[0m     \u001B[0mfullSet\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrainFiles\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mtestFiles\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-1-e4acd8574dac>\u001B[0m in \u001B[0;36mget_files\u001B[0;34m(path, fullPath)\u001B[0m\n\u001B[1;32m    118\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mget_files\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfullPath\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 120\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mfullPath\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mf\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mf\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mnext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwalk\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    121\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mStopIteration\u001B[0m: "
     ]
    }
   ]
  }
 ]
}
